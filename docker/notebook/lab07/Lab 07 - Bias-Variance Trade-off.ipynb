{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Lab 07 - Bias-Variance Trade-off"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the past weeks we discussed different regression and classification algorithms. For many, we have seen that rather than a single model, we are in fact able to fit a family of models:\n",
    "- In the case of polynomial fitting, we can fit different models by changing the polynomial degree $k$. When we increased $k$ out models became more elaborate and were able to fit closer to the data.\n",
    "- In the case of decision trees, we can determine the maximum tree depth. Deeper trees form a higher resolution of partitioning of the feature space, enabling finer classification.\n",
    "- In the case of the $k$-NN classifier, prediction is made based on a neighborhood of size $k$. \n",
    "\n",
    "For all of these examples, we have built the intuision that it order to fit a good model for our traning data we desire a more complex model. However, on the other hand, we have seen that more complex models suffer from high losses over new test datasets. To formalize these two oppositng forces we defined the following:\n",
    "\n",
    "- The **bias** of an estimator $\\widehat{\\theta}$ of some parameter $\\theta$ is its expected deviation from the true value: $Bias\\left(\\widehat{\\theta}\\right) = \\mathbb{E}\\left[\\widehat{\\theta}\\right] - \\theta$.\n",
    "- The **variance** of an estimator $\\widehat{\\theta}$ of some parameter $\\theta$ is the expected values of the squared sampling deviations: $var\\left(\\widehat{\\theta}\\right)=\\mathbb{E}\\left[\\left(\\widehat{\\theta}-\\mathbb{E}\\left[\\widehat{\\theta}\\right]\\right)^2\\right]$.\n",
    "\n",
    "\n",
    "Then we saw that we can decompose the generalization eror of a learner into two parts: the bias and the variance:\n",
    "\n",
    "$$ L_\\mathcal{D} \\left(h_S\\right) = \\underset{bias}{\\underbrace{L_\\mathcal{D}\\left(h^*\\right)}} + \\underset{variance}{\\underbrace{L_\\mathcal{D}\\left(h_S\\right) - L_\\mathcal{D}\\left(h^*\\right)}} $$ \n",
    "\n",
    "where $h^* = argmin_{h\\in\\mathcal{H}} L_\\mathcal{D}\\left(h\\right)$ and $h_S=\\mathcal{A}\\left(S\\right)$, for $S$ the training set. Specifically when the loss function is the MSE function, the generalization loss is decomposted as:\n",
    "\n",
    "$$ \\mathbb{E}\\left[\\left(\\widehat{y}-y^*\\right)\\right] = \\mathbb{E}\\left[\\left(\\widehat{y}-\\overline{y}\\right)^2\\right] + \\left(\\overline{y}-y^*\\right)^2 = var\\left(\\widehat{y}\\right) + Bias^2\\left(\\widehat{y}\\right) $$\n",
    "\n",
    "where $\\overline{y} = \\mathbb{E}\\left[\\widehat{y}\\right]$.\n",
    "\n",
    "\n",
    "In the following lab we will visualize the bias-variance trade-off in their influence on a classifier's decision boundary as well as construct a bias-variance graph over some dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append(\"../\")\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree, export_text\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generation\n",
    "\n",
    "For the purpose of this lab, let us define $\\mathbb{R}^2$ the feature splace where a data-point's class is $1$ if it is in one of three rectangles. Otherside it has a class of $0$. As our dataset we take a the points forming a grid as seen in the figure below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_labels(X):\n",
    "    return np.array([1 if ((1 <= x[0] <= 4.25 and 1 <= x[1] <= 5.25)  or \n",
    "                           (6 <= x[0] <= 9 and 4 <= x[1] <= 9)  or \n",
    "                           (.75 <= x[0] <= 1.75 and 8.1 <= x[1] <= 9.4))\n",
    "                     else 0 for x in X])\n",
    "\n",
    "surface_range = [-.5,10.5], [-.5,10.5]\n",
    "xx, yy = np.meshgrid(np.linspace(0, 10, 25), np.linspace(0, 10, 25))\n",
    "X = np.c_[xx.ravel(), yy.ravel()]\n",
    "y_ = true_labels(X)\n",
    "\n",
    "go.Figure([decision_surface(true_labels, *surface_range), \n",
    "           go.Scatter(x = X[:, 0], y=X[:, 1], mode = 'markers', marker=dict(color=y_, colorscale=custom), showlegend=False)],\n",
    "          layout=go.Layout(title=r\"$\\text{(1) True Data Layout}$\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate Nosy Sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we was to model the noisiness of the sample. Recall, that when modeling \n",
    "By Add noise to data, by changing the classification of a point with probability p.\\\n",
    "The dataset (in avarage) will have m*p points that are classified wrong.\\\n",
    "Implementation note: Binomial distribution with n=1 is Bernoulli distribution.\n",
    "\n",
    "In the plot, the backround is the true data classification. Note that around m*p are in the wronge color - noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(y, p=.1):\n",
    "    return np.array([y, 1-y]).T[range(len(y)), np.random.binomial(1, p, len(y))]\n",
    "\n",
    "train_idx, noise_level = np.random.choice(range(len(X)), 200, replace=False), .25\n",
    "\n",
    "X_train, y_train = X[train_idx], y_[train_idx]\n",
    "y_noisy = add_noise(y_train, noise_level)\n",
    "\n",
    "go.Figure([decision_surface(true_labels, *surface_range),\n",
    "           go.Scatter(x = X_train[:,0], y=X_train[:, 1], mode = 'markers', marker = dict(color=y_noisy, colorscale=custom))],\n",
    "         layout= go.Layout(title=rf\"$\\text{{(2) True Data Layout With Noisy Sample - }}p={noise_level}$\", xaxis_title=\"x\", yaxis_title=\"y\"))\\\n",
    ".show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision bounderies\n",
    "\n",
    "To visualize a classifier's prediction let us plot its decision boundaries over the feature space $\\mathbb{R}^2$. The learner we will be using is of decision trees. Recall that a decision tree is a partitioning of the feature space into a disjoint union of \"boxes\". Then, the prediction made for a new sample $\\mathbf{x}\\in\\mathbb{R}^2$ is by the majoiry vote of the \"box\" in falls into. Insead of only looking at the $argmax$ label of a box, let us visualize the proportion of positives out of all data-points in the box. So: $$ prediction\\left(B\\right) = \\frac{\\left|\\left\\{x\\in B | class(x) = 1\\right\\}\\right|}{\\left|B\\right|} $$\n",
    "\n",
    "This provides us with a continuous scale in the range of $\\left[0,1\\right]$ with the meaning of \"how likely is this box positive\".\n",
    "\n",
    "Below we visualize the decision bounderies of decision tree classifiers of increasing depth.\n",
    "- Run the following block with `visualize_noisy_data = False` to see decision bounderies of the taken sample but with its true labels.\n",
    "- Then run the following block with `visualize_noisy_data = True` to see decision bounderies of the taken sample but with the added noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier(model_type=\"tree\", **kwargs):\n",
    "    if model_type == \"tree\":\n",
    "        return DecisionTreeClassifier(**kwargs)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_noisy_data = False\n",
    "\n",
    "frames = []\n",
    "for k in range(1, 15):\n",
    "    x, y, title = (X_train, y_noisy, \"Noisy Sample\") if visualize_noisy_data else (X_train, y_train, \"True Layout\")\n",
    "    \n",
    "    clf = classifier(\"tree\", max_depth=k).fit(x, y)\n",
    "    frames.append(go.Frame(data = [decision_surface(lambda x: clf.predict_proba(x)[:,1], *surface_range),\n",
    "                                   go.Scatter(x = x[:,0], y=x[:, 1], mode = 'markers', \n",
    "                                              marker = dict(color=y, colorscale=custom))],\n",
    "                           traces=[0],\n",
    "                           layout = go.Layout(title=rf\"$\\text{{(3) Decision Boundary Of {title} - Max Depth }}k={k}$\")))\n",
    "\n",
    "go.Figure(data = frames[0][\"data\"], frames=frames[1:], \n",
    "          layout = go.Layout(title=frames[0][\"layout\"][\"title\"],\n",
    "                             updatemenus=[dict(type=\"buttons\", \n",
    "                                               buttons=[AnimationButtons.play(), AnimationButtons.pause()])])).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that when fitting the model over the true data layout, the algorithm converges into a specific tree of depth $8$. Once it reaches this depth, the tree does not change even when increasing the `max_depth`. At this point it has reaches a training error of zero. \n",
    "\n",
    "Unlines the previous case, if running over the noisy sample, the fitted trees continue to change until a much deeper depth. At the end it achieves a training error of zero, but the outputted model is very complex. We expect this model to perform poorly over a new test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing Bias and Variance Of Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the fitted model (of any learning algorithm) is returned over a given **specific** and **noisy** sample, this model is in fact a random variable. As such also its predictions are **random variables**.\n",
    "\n",
    "Let us calculate the bias and variance properties of a decision tree of increasing depth. Recall the bias and variance definitions:\n",
    "$$ Bias\\left(\\widehat{\\theta}\\right) = \\mathbb{E}\\left[\\widehat{\\theta}\\right] - \\theta $$\n",
    "$$ var\\left(\\widehat{\\theta}\\right)=\\mathbb{E}\\left[\\left(\\widehat{\\theta}-\\mathbb{E}\\left[\\widehat{\\theta}\\right]\\right)^2\\right] $$\n",
    "\n",
    "In both of these definitions, the expectation is over the selection of training sets $S\\sim\\mathcal{D}^m$. For each such set we produce an estimator $\\widehat{\\theta}$, which we then use to make the predictions.\n",
    "\n",
    "So to calculate these measures we will take the training sampled used in figure 2. We will refer to `X_train, y_train` as the ground truth. Then, we change the labels vector `y_train` to add noise and fit a decision tree of a given depth. We will repeat this process multiple times to aquire many estimator $\\widehat{\\theta}_1,\\ldots,\\widehat{\\theta}_T$. Averaging over these estimators will provide us with the empirical expectation needed for both the bias and variance calculations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "frames, ks = [], [1, 2, 8, 16]\n",
    "for i in range(10):\n",
    "    x, y = X_train, add_noise(y_train, 0.25)\n",
    "    data = []\n",
    "    for k in ks:\n",
    "        clf = classifier(\"tree\", max_depth=k).fit(x, y)\n",
    "        data.extend([\n",
    "            decision_surface( lambda x: clf.predict_proba(x)[:,1], *surface_range, showscale=False, dotted=False, density=40),\n",
    "            go.Scatter(x = x[:,0], y=x[:, 1], mode = 'markers', marker = dict(color=y, colorscale=custom), showlegend=False)\n",
    "            ])\n",
    "    frames.append(go.Frame(data = data, name=i,\n",
    "                           traces = list(range(len(data))),#list(range(0, len(data), 2)),\n",
    "                           layout = go.Layout(title=rf\"$\\text{{(4) Decision Boundaries - Iteration {i+1}}}$\")))\n",
    "\n",
    "\n",
    "fig = make_subplots(rows=2, cols=2, subplot_titles=[rf\"$k={k}$\" for k in ks])\n",
    "fig.add_traces(data=frames[0][\"data\"], rows=[1,1,1,1,2,2,2,2], cols=[1,1,2,2,1,1,2,2])\n",
    "fig.update(frames=frames)\n",
    "fig.update_layout(title=r\"$\\text{(4) Decision Boundaries}$\",updatemenus=[dict(type=\"buttons\", buttons=[AnimationButtons.play(), AnimationButtons.pause()])])\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can we learn from figure 4? We begin with understanding the setup presented in the figure. \n",
    "- In all panels and animaiton frames we ovserve, the same data-points shown in figure 2. These stay constant across the figure.\n",
    "- Between each animation frame the data-points are given a new label with probability $p=0.25$. This represents the noisiness of the data. All figure panels use the same noisy data in a given animation frame.\n",
    "- In each panel a decision tree of different depth is fitted, and is represented by its decision boundary.\n",
    "\n",
    "\n",
    "\n",
    "As each animation frame represents fitting a model over a dataset where the $X$ remains the same and the $y$ changes by a bit we are able to **observe** the bias and variance of the predictors.\n",
    "- Notice that the models in the top row are **too simple**. They are not able to capture the complexity of the data. The hypothesis clesses used in each of the two top panels are: decision trees of depth at most 1 (left; i.e. decision stumps) and 2. There are simply no good eoungh hypotheses **in the hypothesis classes** to match the data. The best estimator the algorithm can find is sill \"far\" from the true parameters. So the **bias** observed in the top panels is **high**.\n",
    "\n",
    "\n",
    "- In the bottom row the hypothesis classes are sufficiently large to include trees of the correct depth for this data. As such the **bias** of the estimators retrieved by the learning algorithm in the bottom row is **low**. \n",
    "\n",
    "\n",
    "- Next, consider how consistent a prediction is on average. In the top row, thoguh the decision doundaries move between the different frames, each data-points recieves mostly the same prediction. This change in predicted labels is a proxy to the estimator's variance. As the predictions are mostly the same, the estimator's **variance** is **low**.\n",
    "\n",
    "\n",
    "- Now, when we look at the bottom row we notice that in each frame the decision boundaries change dramatically. Each time they capture small different subsets of the data. Each time the estimated decision tree \"manages\" to correctly classify more of the data-points, even though the labels of each point might change between the animation frames. This means that the estimators seen in the different frames are very different from one another. As such, the **variance** of the estimators is **high**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias-Variance Trade-off"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, after visualizing the effects of the bias and variance thoguh the decision doundaries of the classifiers, let us calculate the bias and variance measurements of the estimator and plot the bias-variance tradeoff graph. To calculate these measures over a given dataset we will calculate the empirical bias and variances of the estimator.\n",
    "\n",
    "We begin with computing the $Bias^2$, $Variance$ and $MSE$ of a given model: a decision tree classifier of depth at most $k=6$.\n",
    "\n",
    "*Note that we are using the estimator's prediction of th eprobability being classified as $1$ and not the actual $0$ or $1$ classification prediction*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_variance(y_hats, y_true):\n",
    "    point_var = np.var(y_hats - y_true, axis=0, ddof=1)\n",
    "    var = np.mean(point_var)\n",
    "    return var\n",
    "\n",
    "\n",
    "def calc_bias(y_hats, y_true):\n",
    "    E_y = y_hats.mean(axis=0)\n",
    "    bias = np.mean((E_y - y_true)**2)\n",
    "    return bias**2\n",
    "\n",
    "\n",
    "def calc_MSE(y_hats, y_true):\n",
    "    mse = np.mean((y_hats - y_true)**2)\n",
    "    return mse\n",
    "\n",
    "\n",
    "def run_simulations(model, X, y, X_test, y_test, p, repetitions):\n",
    "    preds = np.zeros((repetitions, len(y_test)))\n",
    "    for i in range(repetitions):\n",
    "        # Fit a model over noised training set\n",
    "        clf = model.fit(X, add_noise(y, p))\n",
    "        # Predict probabilty of classifying to class 1 over test set\n",
    "        preds[i] = clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    return calc_bias(preds, y_test), calc_variance(preds, y_test), calc_MSE(preds, y_test)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p, repetitions = .25, 500\n",
    "\n",
    "model = classifier('tree', max_depth=8)\n",
    "bias, variance, MSE = run_simulations(model,  \n",
    "                                      X[train_idx], y_[train_idx], \n",
    "                                      X[~train_idx], y_[~train_idx],\n",
    "                                      p, repetitions)\n",
    "\n",
    "print(\"Bias:\\t\\t\\t\", bias)\n",
    "print(\"Variance:\\t\\t\", variance)\n",
    "print(\"Bias^2 + Variance:\\t\", bias + variance)\n",
    "print(\"MSE:\\t\\t\\t\", MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us repeat the process above for different values of $k$ to create the full bias-variance trade-off graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_title_and_name(model_type):\n",
    "    if model_type == \"tree\":\n",
    "        return \"Tree Depth\", \"Desicion Tree\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = list(range(1, 15))\n",
    "x = list(range(len(vals)))\n",
    "x_title, name = get_title_and_name('tree')\n",
    "\n",
    "bias, variance, mse = np.zeros(len(vals)), np.zeros(len(vals)), np.zeros(len(vals))\n",
    "for i, param in enumerate(vals):\n",
    "    model = classifier(\"tree\", max_depth=param) \n",
    "    bias[i], variance[i], mse[i] = run_simulations(model, \n",
    "                                                   X[train_idx], y_[train_idx], \n",
    "                                                   X[~train_idx], y_[~train_idx], \n",
    "                                                   p, repetitions)\n",
    "\n",
    "\n",
    "go.Figure([\n",
    "    go.Scatter(x=x, y=variance, mode='markers + lines', name=r'$Variance$'),\n",
    "    go.Scatter(x=x, y=bias, mode='markers + lines', name=r'$Bias^2$'),\n",
    "    go.Scatter(x=x, y=mse, mode='markers + lines', name=r'$MSE$'),\n",
    "    go.Scatter(x=x, y=np.array(bias) + np.array(variance), mode='markers + lines', name=r'$Bias^2 + Variance$')])\\\n",
    ".update_layout(title=rf\"$\\text{{(5) Bias-Variance Graph Of {name} }}$\", \n",
    "               xaxis = dict(title=x_title, tickvals=x, ticktext=vals)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time To Think...\n",
    "\n",
    "In this lab we have seen a very important phenomina that is observed in many of the algorithms seen in the course, and that is also encountered in many real-life machine learning problems. Let us view the bias-variance trade-off also in the $k$-Nearest-Neighbors classifier. Make the following modification to the code above:\n",
    "\n",
    "1. Add $k$-NN model to the `classifier` function:\n",
    "```\n",
    "    if model_type == 'knn':\n",
    "        return KNeighborsClassifier(kwargs['k'])\n",
    "```\n",
    "\n",
    "2. Add the following to the `get_title_and_name` function:\n",
    "```\n",
    "    if model_type == \"knn\":\n",
    "        return \"K - Number of Neighbors\", \"K-NN\"   \n",
    "```\n",
    "3. Change the block of code above to call the $k$-NN classifier instead of the decision tree classifier. In addition make sure that model complexity increases as we move to the right on the x-axis.\n",
    "        \n",
    "<br>After computing the bias-variance tradeoff graph:\n",
    "1. Is there any meaningfull qualitative difference between the two classifiers ($k$-NN and decision boundaries)?\n",
    "2. What is this difference and how can you explain it? \n",
    "3. How can you explain it?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
